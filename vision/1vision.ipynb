{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autor: Thomas Fetter\n",
    " # 1. üìò What is Machine Vision?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Vision is a subfield of computer science and engineering that enables machines to interpret and process visual information from the world, much like human vision. It is a core technology in areas such as industrial automation, robotics, quality control, autonomous vehicles, and medical imaging.\n",
    "\n",
    "At its core, Machine Vision combines several disciplines:\n",
    "- **Computer Vision**: Algorithms and models that allow machines to understand and analyze images or videos.\n",
    "- **Image Processing**: Techniques for enhancing, filtering, and transforming images to extract meaningful information.\n",
    "- **Artificial Intelligence (AI)** and **Machine Learning (ML)**: Used to classify, detect, or segment objects within visual data.\n",
    "\n",
    "Some common tasks in Machine Vision include:\n",
    "- Image acquisition (via cameras or sensors)\n",
    "- Preprocessing (e.g., filtering, normalization)\n",
    "- Feature extraction\n",
    "- Object detection and recognition\n",
    "- Classification\n",
    "- 3D reconstruction\n",
    "- Visual inspection and quality control\n",
    "\n",
    "Machine Vision systems typically follow a pipeline like:\n",
    "\n",
    "$$\n",
    "\\text{Image Acquisition} \\rightarrow \\text{Preprocessing} \\rightarrow \\text{Feature Extraction} \\rightarrow \\text{Decision Making}\n",
    "$$\n",
    "\n",
    "Unlike biological vision, Machine Vision is designed for **specific** tasks, often with **high speed**, **repeatability**, and **accuracy** in controlled environments.\n",
    "\n",
    "üß† Fun Fact: While the terms *Machine Vision* and *Computer Vision* are often used interchangeably, Machine Vision typically refers to **industrial applications** where the full pipeline from image capture to decision is automated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beispielbilder TODO:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"img/farn50.gif\" width=\"200\"></td>\n",
    "<td><img src=\"bilder/schneeflocke.jpg\" width=\"200\"></td>\n",
    "<td><img src=\"bilder/k√ºste.jpg\" width=\"200\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Farnblatt</td>\n",
    "<td>Schneeflocke</td>\n",
    "<td>K√ºstenlinie</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import cv2  # OpenCV\n",
    "\n",
    "# Load the Test-Image TODO: w√§re das array\n",
    "input_image = cv2.imread(\"img/me1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the input image\n",
    "print(\"Input image\")\n",
    "# Bild anzeigen\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# OpenCV nutzt BGR ‚Äì f√ºr plt.imshow brauchen wir RGB\n",
    "image_rgb = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Bild anzeigen\n",
    "plt.imshow(image_rgb)\n",
    "plt.axis(\"off\")  # Achsen ausblenden\n",
    "plt.title(\"Mein Bild\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gaussian filter kernel 3x3\n",
    "kernel = 1.0/16.0 * np.array([[1,2,1], [2,4,2], [1,2,1]])\n",
    "print(\"Gaussian Kernel\\n{}\".format(kernel))\n",
    "\n",
    "#ALLGEMEINER KERNEL mit auto siegma (-1)\n",
    "# 1D-Gaussian-Kernel (5x1)\n",
    "gk_1d = cv2.getGaussianKernel(ksize=5, sigma=-1)\n",
    "# 2D-Gaussian-Kernel durch √§u√üeres Produkt\n",
    "gk_2d = gk_1d @ gk_1d.T\n",
    "#print(gk_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the filter\n",
    "output_image = cv2.filter2D(input_image, -1, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the filter again\n",
    "output_image = cv2.filter2D(output_image, -1, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Output image\")\n",
    "\n",
    "# OpenCV nutzt BGR ‚Äì f√ºr plt.imshow brauchen wir RGB\n",
    "image_rgb = cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Bild anzeigen\n",
    "plt.imshow(image_rgb)\n",
    "plt.axis(\"off\")  # Achsen ausblenden\n",
    "plt.title(\"Mein Bild\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bildsch√§rfung durch Subtraktion des Gaussian Blur\n",
    "\n",
    "Ein h√§ufig genutzter Ansatz zur Bildsch√§rfung basiert auf dem Prinzip:\n",
    "\n",
    "$\n",
    "\\text{Details} = \\text{Originalbild} - \\text{Weichzeichnung}\n",
    "$\n",
    "\n",
    "Dabei wird das gegl√§ttete Bild (z.‚ÄØB. durch einen Gaussian-Blur-Filter) vom Originalbild abgezogen. √úbrig bleiben die **feinen Strukturen** und **Kanten**, die durch das Gl√§tten unterdr√ºckt wurden.\n",
    "\n",
    "Das resultierende \"Detailbild\" hebt also die **hohen Frequenzen** hervor ‚Äì die feinen √úberg√§nge im Bild. Diese lassen sich zur **Bildsch√§rfung** verwenden oder als Basis f√ºr Kantenextraktion nutzen.\n",
    "\n",
    "Das Verfahren ist einfach, aber sehr wirkungsvoll und liefert visuell anschauliche Ergebnisse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warum braucht man PIL/Pillow f√ºr `display()`?\n",
    "\n",
    "In Jupyter-Notebooks kann man mit der Funktion `IPython.display.display()` **Bilder direkt inline anzeigen** ‚Äì aber das funktioniert nur korrekt mit bestimmten Objekttypen.\n",
    "\n",
    "### ‚úÖ Richtig funktioniert es mit:\n",
    "- `PIL.Image.Image` ‚Üí wird automatisch h√ºbsch und korrekt dargestellt\n",
    "\n",
    "### ‚ùå Nicht funktioniert es mit:\n",
    "- `NumPy`-Arrays ‚Üí werden nicht als Bild erkannt\n",
    "- `OpenCV`-Bilder (`cv2.imread()`) ‚Üí sind BGR statt RGB und geben beim `display()` nur ein leeres Objekt zur√ºck\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "# Blurred Image\n",
    "blurred_image = cv2.GaussianBlur(input_image, (5,5), 0)\n",
    "\n",
    "# OpenCV BGR ‚Üí RGB\n",
    "blurred_rgb = cv2.cvtColor(blurred_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# In PIL-Image umwandeln\n",
    "blurred_pil = Image.fromarray(blurred_rgb)\n",
    "\n",
    "# Anzeigen\n",
    "print(\"Blurred Image\")\n",
    "display(blurred_pil)\n",
    "\n",
    "# Differenzbild (Details extrahieren)\n",
    "detailed = np.float32(input_image) - np.float32(blurred_image)\n",
    "detailed = cv2.normalize(detailed, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "detailed_uint8 = np.uint8(detailed)\n",
    "\n",
    "# Auch hier: BGR ‚Üí RGB ‚Üí PIL\n",
    "detailed_rgb = cv2.cvtColor(detailed_uint8, cv2.COLOR_BGR2RGB)\n",
    "detailed_pil = Image.fromarray(detailed_rgb)\n",
    "\n",
    "print(\"Detailed Image\")\n",
    "display(detailed_pil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bilddetails verst√§rken durch Addition der Gl√§ttung\n",
    "\n",
    "Ein eher untypischer, aber interessanter Effekt entsteht, wenn man das gegl√§ttete Bild **zum Originalbild addiert** mit einem Verst√§rkungsfaktor:\n",
    "\n",
    "$\n",
    "\\text{Detailbild} = \\text{Original} + \\beta \\cdot \\text{Blur}\n",
    "$\n",
    "\n",
    "Das betont helle Bildanteile und verst√§rkt gro√üfl√§chige Helligkeit. Im Vergleich zur klassischen Detailextraktion durch Subtraktion ergibt sich ein \"√ºberzeichneter\" Eindruck, n√ºtzlich z.‚ÄØB. als visueller Effekt oder zur Betonung von Strukturen.\n",
    "\n",
    "Hier wurde testweise $\\beta = 5$ verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differenzbild (Details extrahieren)\n",
    "detailed = np.float32(input_image) + 5*np.float32(blurred_image)\n",
    "detailed = cv2.normalize(detailed, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "detailed_uint8 = np.uint8(detailed) # da rgb 255\n",
    "\n",
    "# Auch hier: BGR ‚Üí RGB ‚Üí PIL\n",
    "detailed_rgb = cv2.cvtColor(detailed_uint8, cv2.COLOR_BGR2RGB)\n",
    "detailed_pil = Image.fromarray(detailed_rgb)\n",
    "\n",
    "# OpenCV BGR ‚Üí RGB\n",
    "#input_rgb = cv2.cvtColor(inputImage, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print(\"Original Image\")\n",
    "display(Image.fromarray(cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)))\n",
    "\n",
    "print(\"Detailed Image\")\n",
    "display(detailed_pil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canny-Edge-Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Canny-Kantendetektion ist mehrstufig und nutzt intern tats√§chlich Gradientenoperatoren wie Sobel, aber sie ist viel mehr als nur Sobel oder Prewitt.\n",
    "1. **Gl√§ttung mit einem Gaussian-Filter**  \n",
    "   ‚Üí reduziert Rauschen und verhindert \"falsche\" Kanten\n",
    "2. **Gradientenberechnung** (z.‚ÄØB. mit Sobel)\n",
    "3. **Non-Maximum Suppression**  \n",
    "   ‚Üí unterdr√ºckt unscharfe √úberg√§nge\n",
    "4. **Hysterese-Schwellenwertverfahren**  \n",
    "   ‚Üí verbindet echte Kanten, ignoriert schwache\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìò Sobel-Operator\n",
    "\n",
    "Der **Sobel-Operator** ist ein Kantendetektor, der den Gradienten eines Bildes durch gewichtete Ableitungen in $x$- und $y$-Richtung berechnet.  \n",
    "Er kombiniert eine **Ableitung** mit einer **Gl√§ttung** (durch gewichtete Mittelung), wodurch er **rauschrobuster** ist als z.‚ÄØB. der Prewitt-Operator.\n",
    "\n",
    "#### üîß Sobel-Kernel:\n",
    "\n",
    "$$\n",
    "G_x =\n",
    "\\begin{bmatrix}\n",
    "-1 & 0 & 1 \\\\\\\\\n",
    "-2 & 0 & 2 \\\\\\\\\n",
    "-1 & 0 & 1\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "G_y =\n",
    "\\begin{bmatrix}\n",
    "-1 & -2 & -1 \\\\\\\\\n",
    "0 & 0 & 0 \\\\\\\\\n",
    "1 & 2 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### üìê Gradientenst√§rke:\n",
    "\n",
    "$$\n",
    "G = \\sqrt{G_x^2 + G_y^2}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üìò Prewitt-Operator\n",
    "\n",
    "Der **Prewitt-Operator** ist ein einfacher Kantendetektor, √§hnlich wie Sobel, aber **ohne Gewichtung**.  \n",
    "Er verwendet gleichm√§√üige Differenzen ohne Mittelwertfilterung und ist daher **weniger rauschresistent**, daf√ºr aber **etwas schneller**.\n",
    "\n",
    "#### üîß Prewitt-Kernel:\n",
    "\n",
    "$$\n",
    "G_x =\n",
    "\\begin{bmatrix}\n",
    "-1 & 0 & 1 \\\\\\\\\n",
    "-1 & 0 & 1 \\\\\\\\\n",
    "-1 & 0 & 1\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "G_y =\n",
    "\\begin{bmatrix}\n",
    "-1 & -1 & -1 \\\\\\\\\n",
    "0 & 0 & 0 \\\\\\\\\n",
    "1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### üìê Gradientenst√§rke:\n",
    "\n",
    "$$\n",
    "G = \\sqrt{G_x^2 + G_y^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "# In Graustufen umwandeln\n",
    "gray = cv2.cvtColor(input_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Weichzeichnen mit GaussianBlur zur Rauschreduzierung\n",
    "blurred = cv2.GaussianBlur(gray, (5, 5), 1.4)\n",
    "\n",
    "# Canny-Kantendetektion\n",
    "edges = cv2.Canny(blurred, threshold1=50, threshold2=150)\n",
    "\n",
    "#  Darstellung mit matplotlib\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Original\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(blurred, cmap=\"gray\")\n",
    "plt.title(\"Gaussian Blur\") \n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(edges, cmap=\"gray\")\n",
    "plt.title(\"Canny-Kanten\") \n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Graustufenbild vorbereiten\n",
    "gray = cv2.cvtColor(input_image, cv2.COLOR_BGR2GRAY)\n",
    "blurred = cv2.GaussianBlur(gray, (5, 5), 1.4)\n",
    "\n",
    "# Interaktive Funktion \n",
    "def show_canny(low_threshold, high_threshold):\n",
    "    edges = cv2.Canny(blurred, low_threshold, high_threshold)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(edges, cmap='gray')\n",
    "    plt.title(f\"Canny Edges\\nlow = {low_threshold}, high = {high_threshold}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Interaktive Slider\n",
    "interact(show_canny,\n",
    "         low_threshold=IntSlider(value=50, min=0, max=255, step=1, description='Low Threshold'),\n",
    "         high_threshold=IntSlider(value=150, min=0, max=255, step=1, description='High Threshold'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sobel-Filter ‚Äì Kanten in X- und Y-Richtung erkennen\n",
    "\n",
    "Der **Sobel-Operator** kombiniert eine Ableitung mit einer Gl√§ttung und ist ideal zur Kantenerkennung in horizontaler und vertikaler Richtung:\n",
    "\n",
    "- **Sobel X** erkennt **vertikale Kanten** (also Helligkeitsunterschiede entlang der X-Achse)\n",
    "- **Sobel Y** erkennt **horizontale Kanten**\n",
    "\n",
    "Die verwendete Kernelgr√∂√üe (`ksize=5`) f√ºhrt zu **glatteren Gradienten** und ist robuster gegen√ºber Rauschen als der Standard (`ksize=3`).\n",
    "\n",
    "Durch die getrennte Darstellung von $G_x$ und $G_y$ kann man anschaulich nachvollziehen, wie sich die **Gradientenrichtung** im Bild ergibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. In Graustufen umwandeln\n",
    "input_image_gray = cv2.cvtColor(input_image, cv2.COLOR_BGR2GRAY)\n",
    "input_image_gray_f32 = np.float32(input_image_gray)\n",
    "\n",
    "\n",
    "# 2. Apply sobel filters for X & Y\n",
    "sobel_x_array = cv2.Sobel(input_image_gray_f32, -1, 1, 0, ksize=5)\n",
    "sobel_y_array = cv2.Sobel(input_image_gray_f32, -1, 0, 1, ksize=5)\n",
    "\n",
    "\n",
    "# 3. Umwandeln in uint8 f√ºr Anzeige\n",
    "sobel_x_u8 = cv2.normalize(sobel_x_array, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "sobel_y_u8 = cv2.normalize(sobel_y_array, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "\n",
    "# 4. In PIL-Images konvertieren (Graustufenbild ‚Üí mode='L')\n",
    "sobel_x_image = Image.fromarray(sobel_x_u8, mode='L')\n",
    "sobel_y_image = Image.fromarray(sobel_y_u8, mode='L')\n",
    "\n",
    "\n",
    "# Anzeigen\n",
    "print(\"Sobel X (horizontal)\")\n",
    "display(sobel_x_image)\n",
    "\n",
    "print(\"Sobel Y (vertikal)\")\n",
    "display(sobel_y_image)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradientenst√§rke (Gradient Magnitude)\n",
    "\n",
    "Die kombinierte Kantenst√§rke ergibt sich aus den Einzelrichtungen $G_x$ und $G_y$ durch den euklidischen Betrag:\n",
    "\n",
    "$$\n",
    "G = \\sqrt{G_x^2 + G_y^2}\n",
    "$$\n",
    "\n",
    "Dadurch entsteht ein vollst√§ndiges Kantenerkennungsbild, ohne Schwellen, wie bei Canny.  \n",
    "Diese Berechnung ist die Grundlage f√ºr viele Kantendetektoren und auch f√ºr fr√ºhe CNN-Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradient magnitude\n",
    "grad_magnitude_gray_f32 = np.sqrt(np.square(sobel_x_array) + np.square(sobel_y_array))\n",
    "\n",
    "# Normalize to 0-255\n",
    "norm_grad_magnitude_gray_f32 = cv2.normalize(grad_magnitude_gray_f32, None, alpha = 0, beta = 255.0, norm_type = cv2.NORM_MINMAX, dtype = cv2.CV_32F)\n",
    "\n",
    "# 3. Umwandeln in uint8 f√ºr Anzeige\n",
    "norm_grad_magnitude_gray_u8 = norm_grad_magnitude_gray_f32.astype(np.uint8)\n",
    "\n",
    "# 4. In PIL-Images konvertieren (Graustufenbild ‚Üí mode='L')\n",
    "norm_grad_magnitude_gray_image = Image.fromarray(norm_grad_magnitude_gray_u8, mode='L')\n",
    "\n",
    "print(\"Gradient-Magnitude-Image\")\n",
    "\n",
    "# Display \n",
    "display(norm_grad_magnitude_gray_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Edges to Angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Winkel berechnen (Richtung) in Radiant: -œÄ bis œÄ\n",
    "angle = np.arctan2(sobel_y_array, sobel_x_array)\n",
    "\n",
    "# 2. Normierung: -œÄ‚Ä¶œÄ ‚Üí 0‚Ä¶1 f√ºr Hue\n",
    "angle_norm = (angle + np.pi) / (2 * np.pi)\n",
    "\n",
    "# 3. Verwendung der vorhandenen Magnitude (berechnet vorher)\n",
    "magnitude_norm = cv2.normalize(grad_magnitude_gray_f32, None, 0, 1, cv2.NORM_MINMAX)\n",
    "\n",
    "# 4. HSV-Bild zusammensetzen\n",
    "hsv = np.zeros((*angle.shape, 3), dtype=np.float32)\n",
    "hsv[..., 0] = angle_norm          # Hue = Richtung\n",
    "hsv[..., 1] = 1.0                 # Saturation = voll\n",
    "hsv[..., 2] = magnitude_norm      # Value = Kantenst√§rke\n",
    "\n",
    "# 5. HSV zu RGB konvertieren\n",
    "rgb = cv2.cvtColor((hsv * 255).astype(np.uint8), cv2.COLOR_HSV2RGB)\n",
    "\n",
    "# 6. In PIL-Bild umwandeln und anzeigen\n",
    "gradient_direction_image = Image.fromarray(rgb)\n",
    "print(\"Gradient Direction (Hue = Richtung, Value = St√§rke)\")\n",
    "display(gradient_direction_image)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOG Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personen erkennen mit HOG + SVM\n",
    "\n",
    "Die Histogram of Oriented Gradients (HOG) + Support Vector Machine (SVM) Kombination ist ein klassischer Ansatz zur Objekterkennung.\n",
    "\n",
    "- HOG extrahiert **Kantenorientierungen** und lokale Gradientenverteilungen\n",
    "- Ein vortrainierter SVM-Klassifikator erkennt damit **Personen**\n",
    "- Durch **Non-Maximum Suppression** werden doppelte Boxen entfernt\n",
    "\n",
    "> Diese Methode ist nicht Deep Learning-basiert, aber effizient und gut geeignet f√ºr klassische Bildverarbeitungspipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils.object_detection import non_max_suppression\n",
    "\n",
    "\n",
    "# 1. Kopie anlegen, um Originalbild nicht zu ver√§ndern\n",
    "image_copy = input_image.copy()\n",
    "\n",
    "# 2. HOG + vortrainierter SVM-Personendetektor\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "# 3. Personen erkennen\n",
    "(rects, weights) = hog.detectMultiScale(\n",
    "    image_copy,                 # Eingabebild\n",
    "    winStride=(4, 4),           # Schrittweite des Sliding Window\n",
    "    padding=(8, 8),             # Kontext um das Detektionsfenster\n",
    "    scale=1.05                 # Skalierungsfaktor f√ºr Bildpyramide\n",
    ")\n",
    "\n",
    "# 4. Bounding Boxes in (x1, y1, x2, y2) umwandeln\n",
    "rects_np = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "\n",
    "# 5. Non-Maximum Suppression (doppelte Boxen entfernen)\n",
    "rects_nms = non_max_suppression(rects_np, probs=None, overlapThresh=0.65)\n",
    "\n",
    "# 6. Rechtecke einzeichnen\n",
    "for (x1, y1, x2, y2) in rects_nms:\n",
    "    cv2.rectangle(image_copy, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "# 7. RGB-Konvertierung + Anzeige mit display()\n",
    "image_result_rgb = cv2.cvtColor(image_copy, cv2.COLOR_BGR2RGB)\n",
    "image_result_pil = Image.fromarray(image_result_rgb)\n",
    "\n",
    "print(\"Erkannte Personen mit Bounding Box (HOG + SVM)\")\n",
    "display(image_result_pil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panorama Stitching mit SIFT + RANSAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîé SIFT ‚Äì Scale-Invariant Feature Transform\n",
    "\n",
    "SIFT ist ein klassischer Algorithmus zur Erkennung und Beschreibung lokaler Bildmerkmale, die robust gegen√ºber Skalierung, Rotation, Helligkeit und teilweise sogar Perspektivverzerrung sind.\n",
    "\n",
    "Der Algorithmus funktioniert in mehreren Schritten:\n",
    "\n",
    "1. **Extrema-Erkennung im Skalenraum:** Bild wird in verschiedenen Ma√üst√§ben analysiert (Difference-of-Gaussian).\n",
    "2. **Keypoint-Lokalisierung:** Auswahl stabiler Punkte basierend auf Kontrast und Kantenn√§he.\n",
    "3. **Orientierungszuweisung:** Jedem Keypoint wird eine Hauptorientierung basierend auf Gradienten gegeben.\n",
    "4. **Descriptors:** Lokales Patch um jeden Keypoint wird in Gradienten-Histogramme zerlegt ‚Üí ergibt einen 128D-Deskriptor.\n",
    "\n",
    "‚û°Ô∏è Die Kombination aus Positions-, Orientierungs- und Deskriptordaten erlaubt robustes Matching √ºber verschiedene Bilder hinweg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Test-Image TODO: w√§re das array\n",
    "input_image_aleft_array = cv2.imread(\"img/stacka_left.jpeg\")\n",
    "\n",
    "\n",
    "# 1. SIFT-Objekt erzeugen (ab OpenCV 4.4 wieder verf√ºgbar)\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# 2. Keypoints & Deskriptoren berechnen\n",
    "keypoints, descriptors = sift.detectAndCompute(input_image_aleft_array, None)\n",
    "\n",
    "# 3. Bild mit Keypoints anzeigen\n",
    "img_sift = cv2.drawKeypoints(\n",
    "    input_image_aleft_array, keypoints, None,\n",
    "    flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
    ")\n",
    "\n",
    "# 4. Darstellung\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(cv2.cvtColor(img_sift, cv2.COLOR_BGR2RGB))\n",
    "plt.title(f\"SIFT Keypoints (Anzahl: {len(keypoints)})\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Matching mit SIFT-Deskriptoren zwischen zwei Bildern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßÆ SIFT Feature Matching zwischen zwei Bildern\n",
    "\n",
    "Nachdem in beiden Bildern Keypoints und Deskriptoren extrahiert wurden, k√∂nnen wir sie miteinander vergleichen. Dazu nutzen wir einen Brute-Force Matcher, der f√ºr jeden Deskriptor im ersten Bild den **n√§chstgelegenen Nachbarn** im zweiten Bild sucht.\n",
    "\n",
    "Wir verwenden den **L2-Abstand** (euklidische Distanz), der f√ºr SIFT-Deskriptoren geeignet ist.\n",
    "\n",
    "Optional wird zus√§tzlich **Lowe's Ratio-Test** verwendet, um schlechte Matches zu filtern:  \n",
    "Wenn das Verh√§ltnis zwischen den zwei besten Matches < 0.75 ist, gilt das Match als eindeutig genug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Zweites Bild laden\n",
    "input_image_aright_array = cv2.imread(\"img/stacka_right.jpeg\")\n",
    "if input_image_aright_array is None:\n",
    "    raise FileNotFoundError(\"img/stacka_right.jpeg wurde nicht gefunden!\")\n",
    "\n",
    "# 2. SIFT-Deskriptoren f√ºr beide Bilder berechnen\n",
    "sift = cv2.SIFT_create()\n",
    "kp1, des1 = sift.detectAndCompute(input_image_aleft_array, None)\n",
    "kp2, des2 = sift.detectAndCompute(input_image_aright_array, None)\n",
    "\n",
    "# 3. Matcher initialisieren (Brute-Force mit L2)\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
    "\n",
    "# 4. K-n√§chste Nachbarn finden (k=2 f√ºr Ratio-Test)\n",
    "matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "# 5. Lowe's Ratio-Test anwenden\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "print(f\"Gute Matches: {len(good_matches)}\")\n",
    "\n",
    "# 6. Matches visualisieren\n",
    "match_vis = cv2.drawMatches(\n",
    "    input_image_aleft_array, kp1,\n",
    "    input_image_aright_array, kp2,\n",
    "    good_matches, None,\n",
    "    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    ")\n",
    "\n",
    "# 7. Plot anzeigen\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(cv2.cvtColor(match_vis, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"SIFT Feature Matches (nach Ratio-Test)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÅ RANSAC-basierte Homographie-Sch√§tzung\n",
    "\n",
    "Nicht alle Matches zwischen zwei Bildern sind korrekt, manche sind falsch aufgrund von Rauschen, Wiederholungen oder Bewegung.  \n",
    "RANSAC (Random Sample Consensus) ist ein robuster Sch√§tzer, der mit solchen Ausrei√üern umgehen kann.\n",
    "\n",
    "**Ablauf von RANSAC f√ºr Homographie:**\n",
    "\n",
    "1. W√§hle zuf√§llig 4 Punktpaare aus den Matches.\n",
    "2. Berechne die Homographiematrix $H$ aus diesen 4 Paaren.\n",
    "3. Wende $H$ auf alle Punkte an und pr√ºfe, wie gut die transformierten Punkte zu den tats√§chlichen Punkten passen (Abstand).\n",
    "4. Z√§hle, wie viele Paare als \"Inlier\" gelten (z.‚ÄØB. Abstand < 3 Pixel).\n",
    "5. Wiederhole viele Male (z.‚ÄØB. 1000 Iterationen).\n",
    "6. W√§hle die $H$ mit den meisten Inliers und berechne daraus die finale Homographie.\n",
    "\n",
    "‚û°Ô∏è Diese Matrix $H$ kann sp√§ter verwendet werden, um das zweite Bild in das erste einzustitchen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extrahiere die korrespondierenden Punkte aus den Matches\n",
    "pts1 = np.float32([kp1[m.queryIdx].pt for m in good_matches])\n",
    "pts2 = np.float32([kp2[m.trainIdx].pt for m in good_matches])\n",
    "\n",
    "# 2. Homographie mit RANSAC berechnen\n",
    "H, mask = cv2.findHomography(pts2, pts1, cv2.RANSAC, ransacReprojThreshold=4.0)\n",
    "\n",
    "# 3. Maske gibt an, welche Matches als Inlier gelten\n",
    "matches_mask = mask.ravel().tolist()\n",
    "\n",
    "print(f\"Gefundene Inliers: {sum(matches_mask)} / {len(matches_mask)}\")\n",
    "\n",
    "# 4. Matches mit Inliers visualisieren\n",
    "match_vis_inliers = cv2.drawMatches(\n",
    "    input_image_aleft_array, kp1,\n",
    "    input_image_aright_array, kp2,\n",
    "    good_matches, None,\n",
    "    matchColor=(0, 255, 0),              # gr√ºn = inliers\n",
    "    singlePointColor=(255, 0, 0),\n",
    "    matchesMask=matches_mask,\n",
    "    flags=cv2.DrawMatchesFlags_DEFAULT\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(cv2.cvtColor(match_vis_inliers, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"RANSAC-Inliers f√ºr Homographie\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßµ Bildwarping & Panorama-Stitching\n",
    "\n",
    "Die berechnete Homographiematrix $H$ beschreibt, wie Punkte aus dem zweiten Bild (`img2`) perspektivisch auf das erste Bild (`img1`) abgebildet werden.\n",
    "\n",
    "Daher k√∂nnen wir mit `cv2.warpPerspective(...)` das zweite Bild so transformieren, dass es auf das Koordinatensystem des ersten Bildes passt.\n",
    "\n",
    "Um das Panorama zu erstellen:\n",
    "1. **Transformiere** das zweite Bild mit $H$ auf die Fl√§che des ersten.\n",
    "2. **Erzeuge ein leeres Gesamtbild**, das beide Bilder aufnehmen kann.\n",
    "3. **Kopiere das erste Bild** direkt ins Panorama.\n",
    "4. **Lege das transformierte Bild** dar√ºber."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schritt 1: Dimensionen der Bilder\n",
    "h1, w1 = input_image_aleft_array.shape[:2]\n",
    "h2, w2 = input_image_aright_array.shape[:2]\n",
    "\n",
    "# Schritt 2: Ecken des zweiten Bildes transformieren\n",
    "corners_img2 = np.float32([[0, 0], [0, h2], [w2, h2], [w2, 0]]).reshape(-1, 1, 2)\n",
    "transformed_corners = cv2.perspectiveTransform(corners_img2, H)\n",
    "\n",
    "# Schritt 3: Grenzen des neuen Panoramabildes berechnen\n",
    "all_corners = np.concatenate((transformed_corners, np.float32([[0, 0], [0, h1], [w1, h1], [w1, 0]]).reshape(-1, 1, 2)), axis=0)\n",
    "[x_min, y_min] = np.int32(all_corners.min(axis=0).ravel() - 0.5)\n",
    "[x_max, y_max] = np.int32(all_corners.max(axis=0).ravel() + 0.5)\n",
    "\n",
    "translation = [-x_min, -y_min]\n",
    "width = x_max - x_min\n",
    "height = y_max - y_min\n",
    "\n",
    "# Schritt 4: Zweites Bild warpen\n",
    "H_translation = np.array([[1, 0, translation[0]], [0, 1, translation[1]], [0, 0, 1]])  # f√ºr Offset-Korrektur\n",
    "result = cv2.warpPerspective(input_image_aright_array, H_translation @ H, (width, height))\n",
    "\n",
    "# Schritt 5: Erstes Bild einf√ºgen\n",
    "result[translation[1]:translation[1] + h1, translation[0]:translation[0] + w1] = input_image_aleft_array\n",
    "\n",
    "# Ergebnis anzeigen\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Finales Panorama\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Voraussetzungen f√ºr gutes Panorama Stitching mit SIFT & RANSAC\n",
    "\n",
    "Damit zwei Bilder erfolgreich zu einem Panorama zusammengesetzt werden k√∂nnen, sollten sie folgende Eigenschaften aufweisen:\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. üîÑ √úberlappung\n",
    "\n",
    "Die Bilder sollten sich in mindestens **30‚Äì50‚ÄØ%** ihrer Fl√§che √ºberlappen.  \n",
    "SIFT kann nur dort arbeiten, wo **gemeinsame Merkmale** in beiden Bildern sichtbar sind.\n",
    "\n",
    "üìå _Je mehr √úberlappung, desto stabiler die Homographie._\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. üí° √Ñhnliche Belichtung & Sch√§rfe\n",
    "\n",
    "Beleuchtung, Kontrast und Bildsch√§rfe sollten **vergleichbar** sein.  \n",
    "Gro√üe Unterschiede k√∂nnen zu fehlerhaften oder fehlenden Matches f√ºhren.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. üß≠ Perspektive & Kamerabewegung\n",
    "\n",
    "Die Kamera sollte sich idealerweise nur leicht **drehen oder schwenken**.  \n",
    "Starke Translation oder Bewegung auf das Objekt zu bzw. davon weg kann zu fehlerhafter Geometrie f√ºhren.\n",
    "\n",
    "üìå _Am besten geeignet: Rotation um die y-Achse (z.‚ÄØB. mit Stativ)._\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. üî≥ Strukturierte Szene\n",
    "\n",
    "Die Szene sollte **viele visuelle Merkmale** enthalten ‚Äì wie Ecken, Kanten, Muster, Texturen.  \n",
    "SIFT braucht Details, um stabile Keypoints zu finden.\n",
    "\n",
    "‚õî Vermeide:\n",
    "- glatte Fl√§chen (z.‚ÄØB. Himmel, wei√üe W√§nde)\n",
    "- sich bewegende Objekte (z.‚ÄØB. Menschen, Autos)\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. üö´ Keine starken Verzerrungen\n",
    "\n",
    "Starke Weitwinkel- oder Fischaugenverzerrung kann die Homographie zerst√∂ren.  \n",
    "Solche Bilder sollten vorher entzerrt werden (z.‚ÄØB. mit `cv2.undistort(...)` und Kamerakalibrierung).\n",
    "\n",
    "---\n",
    "\n",
    "### üéÅ Bonus-Tipp:\n",
    "\n",
    "Wenn du selbst Bilder aufnehmen m√∂chtest:\n",
    "\n",
    "- Nutze das **Querformat**\n",
    "- Bewege die Kamera **gleichm√§√üig mit √úberlapp**\n",
    "- **Vermeide Bewegung in der Szene** (z.‚ÄØB. Personen)\n",
    "- Halte **Horizont und Bildachse konstant**\n",
    "\n",
    "‚û°Ô∏è So k√∂nnen saubere, robuste Stitching-Ergebnisse erzielt werden!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Viola-Jones Face Detection\n",
    "\n",
    "Das Viola-Jones-Verfahren ist ein klassischer Algorithmus zur schnellen und zuverl√§ssigen **Gesichtserkennung in Echtzeit**.  \n",
    "Es wurde 2001 von Paul Viola und Michael Jones vorgestellt und war der erste Echtzeit-Detektor f√ºr Gesichter, der auch auf schwacher Hardware funktioniert.\n",
    "\n",
    "#### üöß Grundprinzipien:\n",
    "\n",
    "1. **Haar-Like Features:**  \n",
    "   Merkmale wie Kanten, Kantenverl√§ufe, dunkle/helle Regionen in Rechtecken\n",
    "\n",
    "2. **Integralbild:**  \n",
    "   Optimierte Datenstruktur, die Summen in Rechtecken in **konstanter Zeit** berechnet\n",
    "\n",
    "3. **AdaBoost:**  \n",
    "   Klassifikator-Kombination ‚Äì viele schwache Merkmale ‚Üí starker Entscheidungsbaum\n",
    "\n",
    "4. **Kaskade aus Klassifikatoren:**  \n",
    "   Mehrere Klassifikationsstufen pr√ºfen nur weiter, wenn vorherige \"ja\" sagen  \n",
    "   ‚Üí extrem schnell und effizient\n",
    "\n",
    "#### üì¶ Vorteile:\n",
    "\n",
    "- Echtzeitf√§hig, auch ohne GPU\n",
    "- Einfach zu verwenden\n",
    "- Robust f√ºr Frontalgesichter bei guter Beleuchtung\n",
    "\n",
    "#### ‚ö†Ô∏è Einschr√§nkungen:\n",
    "\n",
    "- Nicht robust gegen√ºber Perspektive oder starker Rotation\n",
    "- Funktioniert schlechter bei Teilverdeckung oder schr√§gen Gesichtern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Graustufenbild erstellen\n",
    "gray = cv2.cvtColor(input_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# 2. Haar-Cascade-Klassifikator laden\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# 3. Gesichter erkennen (angepasste Parameter)\n",
    "faces = face_cascade.detectMultiScale(\n",
    "    gray,\n",
    "    scaleFactor=1.03,         # Feiner skalieren\n",
    "    minNeighbors=2,           # Weniger Nachbarn n√∂tig\n",
    "    minSize=(20, 20),         # Kleinere Gesichter zulassen\n",
    "    maxSize=(30, 30),         # Maximalgr√∂√üe begrenzen\n",
    "    flags=cv2.CASCADE_SCALE_IMAGE\n",
    ")\n",
    "\n",
    "# 4. Bounding Boxes einzeichnen\n",
    "image_with_faces = input_image.copy()\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(image_with_faces, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "# 5. Anzeige (mit display)\n",
    "image_faces_rgb = cv2.cvtColor(image_with_faces, cv2.COLOR_BGR2RGB)\n",
    "image_faces_pil = Image.fromarray(image_faces_rgb)\n",
    "\n",
    "print(f\"Erkannte Gesichter: {len(faces)}\")\n",
    "display(image_faces_pil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hint: warning: man muss sehr mit den Parametern spielen!\n",
    "\n",
    "#### Frontal\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "#### Profil (seitlich)\n",
    "profile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_profileface.xml\")\n",
    "\n",
    "### Grenzen von Viola-Jones\n",
    "Funktioniert nicht zuverl√§ssig bei:\n",
    "\n",
    "-Sonnenbrillen üòé\n",
    "-starker Rotation / schr√§ge Winkel\n",
    "-verdeckten Gesichtern\n",
    "-Liefert keine Gesichtslandmarken (z.‚ÄØB. Augen, Nase, Mund)\n",
    "\n",
    "### Kein modernes CNN ‚Üí erkennt nur ‚Äûklassische Gesichter‚Äú"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klassifikation mit einem Convolutional Neural Network (CNN)\n",
    "\n",
    "Wir trainieren ein CNN mit **PyTorch** auf dem **MNIST-Datensatz**.\n",
    "\n",
    "- **MNIST** enth√§lt 70.000 handgeschriebene Ziffern (0‚Äì9)\n",
    "- Die Bilder sind 28√ó28 Pixel gro√ü und in Graustufen\n",
    "- Ein CNN lernt hier automatisch, relevante Merkmale aus den Bildern zu extrahieren\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# === 1. Transformation: Tensor + Normalisierung auf [0, 1] ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # konvertiert PIL-Bild zu Tensor [0,1]\n",
    "])\n",
    "\n",
    "# === 2. MNIST-Datens√§tze laden ===\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# === 3. DataLoader erstellen ===\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# === 4. Einzelne Bilder anzeigen mit display() ===\n",
    "examples = next(iter(train_loader))\n",
    "images, labels = examples\n",
    "\n",
    "print(\"Beispiele aus dem MNIST-Datensatz:\")\n",
    "for i in range(8):\n",
    "    # Tensor (1, 28, 28) ‚Üí NumPy-Array (28, 28)\n",
    "    image_np = images[i][0].numpy() * 255  # zur√ºckskalieren weil ToTensor() auf [0,‚ÄØ1] normalisiert\n",
    "    image_uint8 = image_np.astype(np.uint8)\n",
    "    image_pil = Image.fromarray(image_uint8, mode='L')  # das sind 8-Bit-Graustufenbilder\n",
    "    \n",
    "    print(f\"Label: {labels[i].item()}\")\n",
    "    display(image_pil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## erstes CNN\n",
    "\n",
    "Wir definieren ein einfaches Convolutional Neural Network (CNN) f√ºr die Klassifikation von handgeschriebenen Ziffern im MNIST-Datensatz.\n",
    "\n",
    "Das Netzwerk besteht aus zwei Faltungsschichten (mit ReLU und MaxPooling), gefolgt von zwei voll verbundenen (Fully-Connected fc) Schichten:\n",
    "\n",
    "- **Faltungsschichten (Conv2d)**: lernen lokale Bildmerkmale wie Linien, Ecken oder √úberg√§nge\n",
    "- **Pooling-Schichten (MaxPool2d)**: reduzieren die Bildgr√∂√üe und machen das Modell robuster gegen√ºber Verschiebungen\n",
    "- **Voll verbundene Schichten (Linear)**: klassifizieren auf Basis der extrahierten Merkmale\n",
    "\n",
    "## üîç Struktur & Parameter im CNN\n",
    "\n",
    "In der Klasse `SimpleCNN` werden mehrere Layer definiert ‚Äì jede Schicht besitzt:\n",
    "\n",
    "- eine **Ausgabe-Dimension** (Shape des Aktivierungs-Tensors nach dieser Schicht)\n",
    "- eine bestimmte Anzahl an **trainierbaren Parametern** (Gewichte)\n",
    "\n",
    "Hier ist die √úbersicht:\n",
    "\n",
    "| Layer               | Output Shape         | Parameter Shape           | Anzahl Parameter |\n",
    "|--------------------|----------------------|---------------------------|------------------|\n",
    "| Conv2d (1‚Üí8, 3x3)  | (8, 28, 28)           | (8, 1, 3, 3)               | 8√ó1√ó3√ó3 = 72     |\n",
    "| MaxPool2d (2x2)    | (8, 14, 14)           | ‚Äì                         | 0                |\n",
    "| Conv2d (8‚Üí16, 3x3) | (16, 14, 14)          | (16, 8, 3, 3)              | 16√ó8√ó3√ó3 = 1.152 |\n",
    "| MaxPool2d (2x2)    | (16, 7, 7)            | ‚Äì                         | 0                |\n",
    "| Flatten            | (784,)                | ‚Äì                         | 0                |\n",
    "| Linear (784‚Üí64)    | (64,)                 | (64, 784) + (64,)          | 50.240           |\n",
    "| Linear (64‚Üí10)     | (10,)                 | (10, 64) + (10,)           | 650              |\n",
    "| **Gesamt:**        | ‚Äì                     | ‚Äì                         | **52.114**       |\n",
    "\n",
    "> Die Conv- und Linear-Schichten enthalten die trainierbaren Gewichte. \n",
    "\n",
    "> Die Shapes der Ausgaben ergeben sich aus dem Bildfluss durch das Netzwerk.  \n",
    "> Die **Parameter-Formen** ergeben sich aus den Layerdefinitionen ‚Äì und sind genau die Werte, die durch Backpropagation gelernt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# === Ein einfaches CNN definieren ===\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Eingabe: (1, 28, 28)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, padding=1)   # ‚Üí (8, 28, 28)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)                                                   # ‚Üí (8, 14, 14)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(8, 16, 3, padding=1)                                       # ‚Üí (16, 14, 14)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)                                                   # ‚Üí (16, 7, 7)\n",
    "\n",
    "        self.fc1 = nn.Linear(16 * 7 * 7, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)  # 10 Klassen: Ziffern 0‚Äì9\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))   # Convolution + ReLU\n",
    "        x = self.pool1(x)           # Max-Pooling\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = x.view(-1, 16 * 7 * 7)  # Flatten\n",
    "        x = F.relu(self.fc1(x))     # Hidden Layer\n",
    "        x = self.fc2(x)             # Output Layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training des CNN\n",
    "\n",
    "Wir trainieren das Modell mit **CrossEntropyLoss** und dem **Adam-Optimizer**.\n",
    "\n",
    "- CrossEntropyLoss ist ideal f√ºr Klassifikation mit mehreren Klassen\n",
    "- Adam ist ein adaptiver Optimierer, der schneller konvergiert als SGD\n",
    "\n",
    "Wir trainieren f√ºr 3 Epochen, das gen√ºgt bei MNIST meist, um erste Erfolge zu sehen.  \n",
    "Nach jeder Epoche wird der Loss und die Genauigkeit auf den Trainingsdaten ausgegeben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ CrossEntropyLoss ‚Äì Klassifikationsfehler bewerten\n",
    "\n",
    "Die `CrossEntropyLoss` ist die Standard-Loss-Funktion bei **Mehrklassen-Klassifikation** in PyTorch.  \n",
    "Sie misst, wie \"weit entfernt\" die Modellvorhersage von der richtigen Klasse ist.\n",
    "\n",
    "---\n",
    "\n",
    "### üî¢ Eingaben:\n",
    "\n",
    "- **`input`**: Rohwerte (Logits) des Modells f√ºr jede Klasse  \n",
    "  ‚Üí z.‚ÄØB. `[1.2, 0.3, -0.5, 2.7, ...]` f√ºr 10 Klassen\n",
    "- **`target`**: Integer-Label (z.‚ÄØB. `3` f√ºr die wahre Klasse ‚Äû3‚Äú)\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Was passiert intern?\n",
    "\n",
    "1. **Softmax** konvertiert die Logits in Wahrscheinlichkeiten:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
    "$$\n",
    "\n",
    "2. Dann berechnet Cross Entropy den negativen Log-Wert der Wahrscheinlichkeit der wahren Klasse:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -\\log(p_{\\text{richtig}})\n",
    "$$\n",
    "\n",
    "‚Üí je h√∂her die Wahrscheinlichkeit der richtigen Klasse, desto **niedriger der Fehler**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Typische Trainingsstrategien im Deep Learning\n",
    "\n",
    "Beim Trainieren von neuronalen Netzen ‚Äì insbesondere Convolutional Neural Networks ‚Äì ist die richtige Trainingsstrategie entscheidend f√ºr gute Generalisierung.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Epochen\n",
    "\n",
    "- Eine **Epoche** bedeutet: Der gesamte Trainingsdatensatz wurde **einmal** durchlaufen.\n",
    "- Meist werden **mehrere Epochen** ben√∂tigt, damit das Modell die Daten richtig lernt.\n",
    "- Aber: Zu viele Epochen k√∂nnen zum **Overfitting** f√ºhren.\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Overfitting vermeiden\n",
    "\n",
    "Overfitting entsteht, wenn das Modell **zu stark an die Trainingsdaten angepasst ist**, aber **schlecht auf neue, unbekannte Daten** reagiert.\n",
    "\n",
    "Strategien zur Vermeidung:\n",
    "\n",
    "- **Early Stopping**: Training wird gestoppt, wenn sich die Testgenauigkeit nicht mehr verbessert\n",
    "- **Regularisierung** (z.‚ÄØB. `Dropout`, `L2-Weight Decay`)\n",
    "- **Data Augmentation**: k√ºnstliches Vergr√∂√üern der Trainingsdaten durch z.‚ÄØB. Rotation, Zoom, Verschiebung\n",
    "\n",
    "---\n",
    "\n",
    "### üìâ Lernrate (Learning Rate)\n",
    "\n",
    "- Die Lernrate (`lr`) bestimmt, wie stark die Gewichte bei jedem Schritt angepasst werden.\n",
    "- Eine **zu hohe** Lernrate kann das Training instabil machen.\n",
    "- Eine **zu niedrige** Lernrate kann das Training unn√∂tig langsam machen oder zu schlechten Ergebnissen f√ºhren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# === 1. Modell instanziieren ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleCNN().to(device)\n",
    "\n",
    "# === 2. Loss und Optimizer definieren ===\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# === 3. Trainingsloop ===\n",
    "n_epochs = 18  # WARNING Too many -> overfitting\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0                  # Startwert f√ºr den Gesamt-Fehler in dieser Epoche, wird unten erh√∂ht\n",
    "    total, correct = 0, 0               # Z√§hler f√ºr Anzahl aller Bilder (total) und korrekt erkannte Vorhersagen (correct) in dieser Epoche\n",
    "\n",
    "    model.train()                       # Schaltet das Modell in den Trainingsmodus\n",
    "    for images, labels in train_loader: # images: Tensor mit Bildern, labels: korrekte Ziffern\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Vorw√§rtsdurchlauf\n",
    "        outputs = model(images)         # F√ºhre das Modell mit dem aktuellen Batch aus\n",
    "        loss = criterion(outputs, labels)   # FBerechne den Fehler zwischen Vorhersage (outputs) und Ziel (labels); criterion ist hier nn.CrossEntropyLoss()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()           # Zur√ºcksetzen der Gradienten aus vorherigen Schritten ‚Äì> sehr wichtig! Sonst w√ºrden sich Gradienten aufaddieren\n",
    "        loss.backward()                 # Berechnet alle Gradienten √ºber Backpropagation\n",
    "        optimizer.step()                # Anpassung der Gewichte gem√§√ü Gradienten -> der eigentliche \"Lernschritt\"\n",
    "\n",
    "        # Statistik sammeln\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)   # Klassenindex mit h√∂chster Wahrscheinlichkeit ‚Üí das ist die Vorhersage\n",
    "        total += labels.size(0)                     # Gesamtbilder\n",
    "        correct += (predicted == labels).sum().item() # Richtige Treffer\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} - Loss: {running_loss:.4f} - Accuracy: {acc:.2f}%\")\n",
    "\n",
    "print(\"Training abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Saliency Map\n",
    "\n",
    "Eine **Saliency Map** zeigt, welche Pixel im Eingabebild am meisten zur Entscheidung des Modells beitragen.\n",
    "\n",
    "- Je heller ein Pixel, desto **st√§rker beeinflusst** es das Ergebnis\n",
    "- Die Map basiert auf den **Gradienten der Ausgabe** bezogen auf die **Eingabe**\n",
    "- Ver√§ndert man diese Pixel, √§ndert sich die Vorhersage besonders stark\n",
    "\n",
    "> So erh√§lt man einen ersten Einblick in das \"Wahrnehmungsfeld\" eines neuronalen Netzes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sicherstellen, dass das Modell im Eval-Modus ist\n",
    "model.eval()\n",
    "\n",
    "# === Ein einzelnes Bild aus dem Testset nehmen ===\n",
    "img = images[0].unsqueeze(0).to(device)  # (1, 1, 28, 28)\n",
    "img.requires_grad_()                     # Aktiviert Gradientenverfolgung f√ºr Input\n",
    "\n",
    "true_label = labels[0].item()\n",
    "\n",
    "# === Vorhersage und Loss berechnen ===\n",
    "output = model(img)\n",
    "loss = criterion(output, torch.tensor([true_label]).to(device))\n",
    "\n",
    "# === Backward: Gradienten berechnen wrt Input ===\n",
    "model.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "# === Saliency: Absolutbetrag der Gradienten des Inputs ===\n",
    "saliency = img.grad.data.abs().squeeze().cpu().numpy()\n",
    "\n",
    "# === Anzeige ===\n",
    "plt.figure(figsize=(6, 3))\n",
    "\n",
    "# Originalbild\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img.squeeze().detach().cpu().numpy(), cmap='gray')\n",
    "plt.title(f\"Originalbild (Ziffer: {true_label})\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Saliency Map\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(saliency, cmap='hot')\n",
    "plt.title(\"Saliency Map\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test & Vorhersage\n",
    "\n",
    "Nach dem Training evaluieren wir das Modell auf den Testdaten:\n",
    "\n",
    "- Die **Testgenauigkeit** zeigt, wie gut das Netzwerk auf unbekannte Bilder generalisiert\n",
    "- Wir zeigen au√üerdem einige zuf√§llige Testbilder mit den **Vorhersagen** des Modells\n",
    "\n",
    "> Dies ist ein klassisches Ende einer Bildklassifikation mit Deep Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Inferenzmodus aktivieren (kein Dropout, kein BatchNorm-Update)\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Kein Gradienten-Tracking n√∂tig\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f\"üîç Test-Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eine Mini-Batch aus Testdaten holen\n",
    "examples = next(iter(test_loader))\n",
    "images, labels = examples\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "# Vorhersagen generieren\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print(\"üì∏ Zuf√§llige Testbilder mit Vorhersage:\")\n",
    "for i in range(8):\n",
    "    img = images[i][0].cpu().numpy() * 255\n",
    "    img_uint8 = img.astype(np.uint8)\n",
    "    img_pil = Image.fromarray(img_uint8, mode='L')\n",
    "\n",
    "    print(f\"Label: {labels[i].item()} | Vorhersage: {predicted[i].item()}\")\n",
    "    display(img_pil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÅ Fazit & Ausblick\n",
    "\n",
    "In diesem Notebook wurde ein vollst√§ndiger Einstieg in die Bildverarbeitung und Mustererkennung gegeben ‚Äì vom klassischen **Sobel-Filter**, √ºber **Kanten- und Merkmalsextraktion**, bis hin zur **modernen Klassifikation mit Convolutional Neural Networks (CNNs)**.\n",
    "\n",
    "### üîç Was haben wir gelernt?\n",
    "\n",
    "- Wie sich Kanten, Gradienten und Strukturen im Bild erkennen lassen\n",
    "- Wie klassische Methoden (Gaussian Blur, Sobel, Canny, HOG, Viola-Jones) funktionieren\n",
    "- Wie ein CNN Bildmerkmale selbstst√§ndig lernt und klassifiziert\n",
    "- Wie man mit `PyTorch` ein eigenes Modell erstellt, trainiert und testet\n",
    "\n",
    "### üß† Was w√§re als n√§chstes spannend?\n",
    "\n",
    "- **Data Augmentation**: zuf√§llige Rotation, Zoom, Verschiebung ‚Üí robusteres Modell\n",
    "- **Dropout / Regularisierung**: √úberanpassung verhindern\n",
    "- **Konfusionsmatrix** und **F1-Score** statt nur Accuracy\n",
    "- **Transfer Learning**: vortrainiertes Modell (z.‚ÄØB. auf ImageNet) auf eigene Daten anwenden\n",
    "- **Eigene Datens√§tze** statt MNIST\n",
    "- **Explainable AI (XAI)**: z.‚ÄØB. mit Grad-CAM visualisieren, was das Modell ‚Äûsieht‚Äú\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
